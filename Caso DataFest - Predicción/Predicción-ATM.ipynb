{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6266d243-b5d9-4e35-bf4d-e93d70b57272",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Predicción de Demanda de Efectivo en Cajeros Automáticos\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"CIMA.jpg\" alt=\"Logo\" width=\"300\">\n",
    "</div>\n",
    "\n",
    "## Introducción\n",
    "\n",
    "El manejo eficiente del efectivo en cajeros automáticos (ATMs) es crucial para garantizar la disponibilidad del servicio y minimizar los costos operativos. Un abastecimiento inadecuado puede resultar en faltantes de efectivo, afectando la satisfacción del cliente y la reputación de la institución financiera. Por otro lado, un abastecimiento excesivo implica costos adicionales en logística y seguridad.\n",
    "\n",
    "Este proyecto se enfoca en predecir la demanda futura de efectivo en ATMs utilizando técnicas avanzadas de aprendizaje automático y reconciliación de pronósticos jerárquicos. El objetivo es optimizar la logística de abastecimiento, reduciendo costos operativos y mejorando la disponibilidad de efectivo para los usuarios.\n",
    "\n",
    "---\n",
    "\n",
    "## Descripción del Dataset\n",
    "\n",
    "El dataset multivariado utilizado en este proyecto contiene registros históricos de transacciones de cajeros automáticos. Las variables clave capturan el comportamiento de la demanda de efectivo y factores que pueden influir en ella.\n",
    "\n",
    "### Variables Principales\n",
    "\n",
    "- **fecha_transaccion**: Fecha en la que se realizó la transacción. Es esencial para identificar patrones temporales y estacionales en la demanda.\n",
    "- **codigo_cajero**: Identificador único del cajero automático (ATM). Permite diferenciar entre distintos puntos de operación.\n",
    "- **tipo_cajero**: Indica el tipo de cajero, categorizado como 'A' o 'B'. Esta variable puede influir en la demanda según la ubicación, características demográficas o uso específico del cajero.\n",
    "- **saldo_inicial**: Monto de efectivo disponible al inicio del día. Proporciona contexto sobre el nivel de efectivo previo a las transacciones.\n",
    "- **demanda**: Cantidad de efectivo retirado por los usuarios. Es la variable objetivo que se busca predecir, ya que representa la demanda real de los clientes.\n",
    "- **abastecimiento**: Monto de efectivo agregado al cajero para reabastecerlo. Es un factor crítico para mantener niveles adecuados de efectivo y evitar faltantes.\n",
    "- **saldo_final**: Cantidad de efectivo restante al final del día, después de considerar la demanda y el abastecimiento.\n",
    "\n",
    "---\n",
    "\n",
    "## Desafío del Proyecto\n",
    "\n",
    "El reto consiste en desarrollar un modelo predictivo que estime con precisión la demanda diaria de efectivo en cada cajero automático. La predicción debe considerar factores temporales, estacionales y comportamentales que afectan la demanda. Además, se busca implementar una reconciliación jerárquica de pronósticos para asegurar coherencia entre diferentes niveles de agregación (por ejemplo, grupos de cajeros, clústeres y el total general).\n",
    "\n",
    "---\n",
    "\n",
    "## Metodología\n",
    "\n",
    "### Preprocesamiento de Datos\n",
    "\n",
    "1. **Análisis Exploratorio de Datos (EDA)**: Se realizó un EDA para entender la distribución de las variables, identificar patrones estacionales y detectar valores atípicos o datos faltantes.\n",
    "2. **Tratamiento de Datos Faltantes**: Se aplicaron técnicas de imputación, como KNN, para rellenar valores faltantes en las variables relevantes.\n",
    "3. **Generación de Características (Feature Engineering)**:\n",
    "   - **Variables Temporales**: Se extrajeron variables como el día de la semana, mes, si es fin de semana o día festivo, y componentes cíclicos (senos y cosenos) para capturar estacionalidades.\n",
    "   - **Lags y Ventanas Móviles**: Se crearon variables de retraso (lags) y medias móviles para capturar dependencias temporales en la demanda.\n",
    "   - **Agrupación de Cajeros**: Se agruparon los cajeros en clústeres utilizando técnicas de clustering (por ejemplo, K-means) basadas en características similares de demanda y comportamiento.\n",
    "\n",
    "### Modelado Predictivo\n",
    "\n",
    "Se probaron varios modelos de aprendizaje automático para predecir la demanda de efectivo:\n",
    "\n",
    "- **LightGBM**: Un modelo de gradient boosting eficiente que maneja grandes conjuntos de datos y captura relaciones no lineales.\n",
    "  - **Optimización de Hiperparámetros**: Se realizó una búsqueda en malla (grid search) para encontrar los mejores hiperparámetros.\n",
    "- **AdaBoost**: Un algoritmo de boosting que combina múltiples modelos débiles para formar un modelo fuerte.\n",
    "  - **Ventajas**: Es robusto a sobreajuste y puede mejorar el rendimiento en conjuntos de datos con ruido.\n",
    "- **Ensamble de Modelos**: Se promediaron las predicciones de LightGBM y AdaBoost para crear un modelo de ensamble que aprovecha las fortalezas de ambos.\n",
    "  - **Justificación**: Los ensambles suelen mejorar la precisión y robustez de las predicciones al combinar diferentes modelos.\n",
    "\n",
    "### Reconciliación de Pronósticos Jerárquicos\n",
    "\n",
    "#### Motivación\n",
    "\n",
    "En sistemas con estructuras jerárquicas, como múltiples cajeros agrupados en clústeres y grupos, es esencial que las predicciones a niveles inferiores sumen coherentemente a las predicciones en niveles superiores. Esto asegura consistencia en las decisiones operativas y evita discrepancias en la planificación.\n",
    "\n",
    "#### Método MinT (Minimum Trace)\n",
    "\n",
    "Se implementó el método MinT (Minimum Trace) para reconciliar los pronósticos en la jerarquía. MinT es una técnica estadística que ajusta las predicciones en diferentes niveles para minimizar el error total, garantizando coherencia entre ellos.\n",
    "\n",
    "**Pasos Clave:**\n",
    "\n",
    "1. **Construcción de la Matriz de Diseño (S)**: Representa las relaciones de agregación entre los diferentes niveles de la jerarquía (Total, Grupo, Clúster, Cajero).\n",
    "2. **Cálculo de la Matriz de Covarianza de Errores**: Se estimó la covarianza de los errores de pronóstico en el nivel más bajo (cajeros individuales) utilizando los errores históricos.\n",
    "3. **Aplicación del Método MinT**: Se ajustaron las predicciones base mediante una fórmula que minimiza la traza de la matriz de covarianza de los errores reconciliados, asegurando que las predicciones en niveles inferiores sumen a las de niveles superiores.\n",
    "\n",
    "**Ventajas del MinT:**\n",
    "\n",
    "- **Coherencia**: Garantiza que las predicciones agregadas en niveles inferiores coincidan con las predicciones en niveles superiores.\n",
    "- **Eficiencia Estadística**: Minimiza el error global de las predicciones reconciliadas.\n",
    "- **Flexibilidad**: Puede aplicarse a jerarquías de cualquier profundidad y estructura.\n",
    "\n",
    "---\n",
    "\n",
    "## Resultados\n",
    "\n",
    "### Evaluación de Modelos\n",
    "\n",
    "- **Métricas Utilizadas**: RMSE (Root Mean Squared Error) y MAPE (Mean Absolute Percentage Error).\n",
    "- **Desempeño**: El modelo de ensamble superó a los modelos individuales, proporcionando predicciones más precisas y estables.\n",
    "\n",
    "### Impacto de la Reconciliación\n",
    "\n",
    "- **Consistencia**: Las predicciones reconciliadas mostraron coherencia entre todos los niveles de la jerarquía.\n",
    "- **Mejora en la Planificación**: Al tener pronósticos consistentes, se facilita la toma de decisiones en logística y abastecimiento.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusiones\n",
    "\n",
    "La implementación de modelos de aprendizaje automático avanzados, combinados con técnicas de reconciliación jerárquica como el método MinT, permite predecir de manera precisa y consistente la demanda de efectivo en cajeros automáticos. Esto tiene un impacto directo en la optimización de la logística de abastecimiento, reducción de costos operativos y mejora en la satisfacción del cliente.\n",
    "\n",
    "### Próximos Pasos:\n",
    "\n",
    "1. **Incorporar Variables Externas**: Considerar factores macroeconómicos o eventos especiales que puedan influir en la demanda.\n",
    "2. **Implementación en Tiempo Real**: Desplegar el modelo en un entorno productivo para predicciones en tiempo real.\n",
    "3. **Monitoreo y Retroalimentación**: Establecer un sistema de monitoreo continuo para ajustar el modelo según nuevos datos y cambios en el comportamiento de los usuarios.\n",
    "\n",
    "---\n",
    "\n",
    "## Autor\n",
    "Andre Saul Juarez Castro - [LinkedIn](https://www.linkedin.com/in/andre-juarez-castro/)   \n",
    "  \n",
    "Jairo Gonzalo Rojas Melgarejo- [LinkedIn](https://www.linkedin.com/in/jairo-gonzalo-rojas-melgarejo-248166239/)   \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6834995e-670f-4c01-be0c-ad1340bd3773",
   "metadata": {},
   "source": [
    "# **Paquetes Utilizados en este Proyecto**\n",
    "\n",
    "En este proyecto, se hace uso de una combinación de paquetes externos descargables y módulos creados específicamente para modularizar el código. A continuación, se presenta una descripción de ambos.\n",
    "\n",
    "---\n",
    "\n",
    "## **Paquetes Externos**\n",
    "Estos paquetes deben descargarse e instalarse antes de ejecutar el proyecto. Se utilizan para realizar análisis, visualización de datos, preprocesamiento, y modelado predictivo.\n",
    "\n",
    "### **Paquetes de Manipulación y Análisis de Datos**\n",
    "- `pandas`: Para la manipulación y análisis de datos en estructuras de DataFrame.\n",
    "- `numpy`: Para realizar cálculos numéricos eficientes y manejo de matrices.\n",
    "\n",
    "### **Paquetes de Visualización**\n",
    "- `matplotlib`: Para crear gráficos estáticos y visualizaciones personalizadas.\n",
    "- `seaborn`: Para visualizaciones estadísticas y gráficos avanzados basados en Matplotlib.\n",
    "\n",
    "### **Paquetes para Modelado Predictivo**\n",
    "- `lightgbm`: Biblioteca para construir modelos predictivos basados en algoritmos de boosting.\n",
    "- `xgboost`: Otro paquete para boosting, útil en tareas de regresión y clasificación.\n",
    "- `scikit-learn`: Conjunto de herramientas para preprocesamiento, modelos predictivos y evaluación.\n",
    "  - `RandomForestRegressor`: Modelo basado en árboles de decisión.\n",
    "  - `LinearRegression`: Regresión lineal simple.\n",
    "  - `RandomizedSearchCV`: Búsqueda aleatoria para optimización de hiperparámetros.\n",
    "  - `StandardScaler`: Para la normalización de características.\n",
    "\n",
    "### **Paquetes de Series de Tiempo**\n",
    "- `tslearn`: Para análisis y clustering de series de tiempo, incluye métricas como DTW.\n",
    "- `statsmodels`: Modelos estadísticos y de series de tiempo como ETS y ARIMA.\n",
    "\n",
    "### **Otros Paquetes Útiles**\n",
    "- `joblib`: Para serializar y guardar modelos en disco.\n",
    "- `skimpy`: Para realizar resúmenes rápidos de los datos.\n",
    "- `scipy`: Contiene herramientas estadísticas, como `uniform` y `randint` para búsqueda de hiperparámetros.\n",
    "- `collections`: Para estructuras de datos avanzadas como `defaultdict`.\n",
    "- `warnings`: Para suprimir o gestionar mensajes de advertencia.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## **Preparación**\n",
    "Antes de comenzar, asegúrate de instalar los paquetes externos ejecutando el siguiente comando:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca057d8f-1c23-4f99-8016-1c733d7e1e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For time series clustering and DTW\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.metrics import dtw\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "\n",
    "# For linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "from skimpy import skim, generate_test_data\n",
    "import seaborn as sns\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "import joblib\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "import lightgbm as lgb\n",
    "from statsmodels.tsa.exponential_smoothing.ets import ETSModel\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from xgboost import XGBRegressor\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f367801f-3e55-4597-ae1c-f7b8ea775eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974cb4fa-30c3-451a-ad91-e482357e77ce",
   "metadata": {},
   "source": [
    "# **Modularización del Proyecto**\n",
    "\n",
    "En este proyecto, el código ha sido modularizado para facilitar su mantenimiento, escalabilidad y reutilización. La modularización consiste en dividir las funcionalidades principales en diferentes archivos `.py`, donde cada archivo representa un módulo especializado. Estos módulos son luego importados y utilizados en el cuaderno Jupyter (`.ipynb`) para realizar el análisis y generar resultados.\n",
    "\n",
    "---\n",
    "\n",
    "## **Descripción de los Módulos**\n",
    "\n",
    "### **`cluster.py`**\n",
    "- Este módulo contiene funciones relacionadas con el agrupamiento de cajeros automáticos (ATMs) mediante técnicas de **clustering**.\n",
    "- Incluye la lógica para realizar agrupaciones basadas en características de transacciones, demanda y comportamientos similares.\n",
    "- **Técnicas utilizadas**: K-means, análisis de características, y visualización de grupos.\n",
    "\n",
    "---\n",
    "\n",
    "### **`graficos.py`**\n",
    "- Encargado de generar visualizaciones para el análisis exploratorio de datos y la presentación de resultados.\n",
    "- Incluye funciones para gráficos de tendencias, distribuciones, series de tiempo y comparaciones entre predicciones y valores reales.\n",
    "- **Librerías utilizadas**: Matplotlib, Seaborn.\n",
    "\n",
    "---\n",
    "\n",
    "### **`modelito.py`**\n",
    "- Este módulo encapsula las funciones relacionadas con el **modelado predictivo**.\n",
    "- Contiene la lógica para entrenar modelos como **LightGBM**, **AdaBoost** y ensamblar predicciones.\n",
    "- También incluye la optimización de hiperparámetros y métricas de evaluación como **RMSE** y **MAPE**.\n",
    "\n",
    "---\n",
    "\n",
    "### **`process.py`**\n",
    "- Responsable del **preprocesamiento de datos**, incluyendo:\n",
    "  - Tratamiento de datos faltantes utilizando métodos como **KNN**.\n",
    "  - Generación de características temporales y derivadas (lags, medias móviles, transformaciones cíclicas).\n",
    "  - Preparación de los datos para alimentar los modelos predictivos.\n",
    "\n",
    "---\n",
    "\n",
    "## **Flujo de Trabajo en el Cuaderno Jupyter**\n",
    "\n",
    "### **Importación de Módulos**\n",
    "Los módulos creados se importan en el archivo Jupyter utilizando la sintaxis `import module_name` o `from module_name import specific_function`.  \n",
    "Esto permite centralizar la lógica en archivos externos y mantener el cuaderno organizado.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee52816-c9cb-46ce-8102-0f093bdeef2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import process\n",
    "import importlib\n",
    "import cluster \n",
    "import graficos\n",
    "importlib.reload(cluster)\n",
    "importlib.reload(process)\n",
    "import process as p\n",
    "import cluster as cl\n",
    "import graficos as gr\n",
    "importlib.reload(gr)\n",
    "import graficos as gr\n",
    "import modelito as md\n",
    "import modelito as md\n",
    "importlib.reload(md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf0a6b1-2363-4d97-a002-18a520ac1bbc",
   "metadata": {},
   "source": [
    "## 1)  Carga del Dataset\n",
    "\n",
    "Se carga el archivo `Datafest24_Train_combined.csv` en un DataFrame utilizando la función `read_csv` de `pandas`. El archivo se lee con un separador de punto y coma (`;`), y el DataFrame resultante se almacena en la variable `df`. A continuación, se muestra el contenido del DataFrame cargado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8cd51a-0a2a-4203-99cc-b81c9078528f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Datafest24_Train_combined.csv\",sep = ';')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23598759-c89e-43ac-96bf-c5c9c91affe4",
   "metadata": {},
   "source": [
    "## 2) Preprocesamiento y Optimización de Tipos de Datos\n",
    "\n",
    "1. **Renombrar Columnas**: Se renombraron algunas columnas del DataFrame original `df` para mejorar su legibilidad y coherencia:\n",
    "   - `fecha_transaccion` a `transactionTime`\n",
    "   - `codigo_cajero` a `atmId`\n",
    "   - `demanda` a `balance_change`\n",
    "\n",
    "2. **Preprocesamiento de Datos**: Se aplicó la función `preprocess_atm_data` de un módulo `p` para procesar y transformar el DataFrame `df`. Luego, se cargó el DataFrame preprocesado desde un archivo CSV llamado `df_preprocessed_final.csv`.\n",
    "\n",
    "3. **Eliminación de Columna No Necesaria**: Se eliminó la columna `Unnamed: 0` del DataFrame para liberar memoria, ya que no contiene información útil.\n",
    "\n",
    "4. **Optimización de Tipos de Datos**: Se creó una función llamada `optimize_data_types` que recorre las columnas del DataFrame y convierte los tipos de datos de las columnas:\n",
    "   - De `float64` a `float32` para reducir el uso de memoria.\n",
    "   - De `int64` a `int32` para optimizar el espacio ocupado.\n",
    "\n",
    "El DataFrame final preprocesado y optimizado se almacena en la variable `df_preprocessed`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f329ba2-b0e0-476e-bd1c-159960e69e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'fecha_transaccion':'transactionTime','codigo_cajero':'atmId','demanda':'balance_change'})\n",
    "df_proc = p.preprocess_atm_data(df)\n",
    "df_proc=pd.read_csv(\"df_preprocessed_final.csv\")\n",
    "df_proc=df_proc.drop([\"Unnamed: 0\"], axis=1)\n",
    "# Para poder salvar memoria\n",
    "def optimize_data_types(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'float64':\n",
    "            df[col] = df[col].astype('float32')\n",
    "        elif df[col].dtype == 'int64':\n",
    "            df[col] = df[col].astype('int32')\n",
    "    return df\n",
    "\n",
    "df_preprocessed = optimize_data_types(df_proc)\n",
    "df_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf57950-ed9b-496f-971f-a914c2b27e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Función para aplicar la transformación Box-Cox\n",
    "def aplicar_boxcox(df, atm_id):\n",
    "    # Filtrar los datos para el ATM específico\n",
    "    atm_data = df[df['atmId'] == atm_id].copy()\n",
    "    \n",
    "    # Asegurarse de que todos los valores de 'balance_change' sean positivos para aplicar Box-Cox\n",
    "    shift = abs(atm_data['balance_change'].min()) + 1\n",
    "    balance_change = atm_data['balance_change'] + shift\n",
    "    \n",
    "    # Aplicar la transformación Box-Cox\n",
    "    balance_change_boxcox, lam = stats.boxcox(balance_change)\n",
    "    \n",
    "    # Guardar la transformación en el DataFrame original\n",
    "    atm_data['balance_change_boxcox'] = balance_change_boxcox\n",
    "    \n",
    "    # Mostrar el valor de lambda\n",
    "    print(f\"Lambda de Box-Cox para el ATM {atm_id}: {lam}\")\n",
    "    \n",
    "    return atm_data, lam, shift\n",
    "\n",
    "# Función para destransformar valores de Box-Cox\n",
    "def destransformar_boxcox(y_boxcox, lam, shift):\n",
    "    if lam != 0:\n",
    "        y_original = (y_boxcox * lam + 1)**(1/lam) - shift\n",
    "    else:\n",
    "        y_original = np.exp(y_boxcox) - shift\n",
    "    return y_original\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc1530c-fa01-4d03-91c0-a9147fac87e1",
   "metadata": {},
   "source": [
    "## 3)  Análisis y Agrupación de Cajeros\n",
    "\n",
    "1. **Filtrado por Tipo de Cajero**: \n",
    "   - Se dividió el DataFrame `df_preprocessed` en dos subconjuntos: `df_a` para los cajeros de tipo 'A' y `df_b` para los cajeros de tipo 'B'. Esto permite realizar análisis específicos para cada tipo de cajero.\n",
    "\n",
    "2. **Extracción de Características Temporales**:\n",
    "   - Se aplicó la función `extract_time_series_features` para extraer características relacionadas con series temporales de las transacciones de cajeros tipo 'A' (`extracted_features_a`) y tipo 'B' (`extracted_features_b`).\n",
    "\n",
    "3. **Determinación del Número Óptimo de Clusters**:\n",
    "   - Se utilizaron dos métodos para determinar el número óptimo de clústeres para los cajeros tipo 'B':\n",
    "     - **Método Davies-Bouldin**: `determine_optimal_clusters_davies_bouldin` evaluó la calidad de la agrupación.\n",
    "     - **Método Silhouette**: `determine_optimal_clusters_silhouette` proporcionó una evaluación alternativa para el número óptimo de clústeres.\n",
    "   - Ambos métodos se probaron con un máximo de 50 clústeres.\n",
    "\n",
    "4. **Análisis Univariado de Series Temporales**:\n",
    "   - Se realizaron análisis univariados para estudiar las series temporales de las transacciones de cajeros. Se utilizó la función `analyze_univariate_time_series` para analizar las transacciones de los cajeros tipo 'B', y se filtró específicamente el cajero con `atmId` igual a '306' para examinar su comportamiento en particular.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0f0988-89df-4713-98cc-9d439e43797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a = df_preprocessed[df_preprocessed['tipo_cajero'] == 'A']\n",
    "df_b = df_preprocessed[df_preprocessed['tipo_cajero'] == 'B']\n",
    "\n",
    "extracted_features_a = cl.extract_time_series_features(df_a)\n",
    "extracted_features_b = cl.extract_time_series_features(df_b)\n",
    "\n",
    "#Determinar el numero exacto  de clusters\n",
    "cl.determine_optimal_clusters_davies_bouldin(extracted_features_b, max_k=50)\n",
    "cl.determine_optimal_clusters_silhouette(extracted_features_b, max_k=50)\n",
    "\n",
    "#Gráficos univariados de una sola serie\n",
    "gr.analyze_univariate_time_series(df_b,df_b[df_b['atmId']=='306'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dde098-242d-4b80-a403-e29b2cd10930",
   "metadata": {},
   "source": [
    "## 4) Agrupación de Cajeros por Clústeres Basados en Características\n",
    "\n",
    "1. **Clustering de Cajeros Tipo A**:\n",
    "   - Se realiza el agrupamiento de cajeros tipo 'A' utilizando las características extraídas en `extracted_features_a` con un número de 8 clústeres, usando la función `perform_feature_based_clustering` de la librería de clustering (`cl`).\n",
    "   - El resultado del agrupamiento (`atm_cluster_map_a`) se fusiona con el DataFrame `df_a` mediante el identificador único `atmId`, y se crea el DataFrame `df_a_clustered` que contiene las etiquetas de clúster para cada cajero.\n",
    "\n",
    "2. **Clustering de Cajeros Tipo B**:\n",
    "   - Se realiza el agrupamiento de cajeros tipo 'B' utilizando las características extraídas en `extracted_features_b` con un número de 12 clústeres, aplicando la misma función `perform_feature_based_clustering`.\n",
    "   - El resultado del agrupamiento (`atm_cluster_map_b`) se fusiona con el DataFrame `df_b` mediante el identificador `atmId`, y se crea el DataFrame `df_b_clustered`.\n",
    "\n",
    "3. **Obtención de Etiquetas de Clúster**:\n",
    "   - Se extraen las etiquetas de los clústeres de ambos DataFrames `df_a_clustered` y `df_b_clustered` para obtener los clústeres únicos presentes en cada grupo de cajeros (`cluster_labels_a` y `cluster_labels_b`).\n",
    "\n",
    "4. **Impresión de las Etiquetas de Clúster**:\n",
    "   - Se imprimen las etiquetas de clúster para los cajeros tipo 'A' y tipo 'B', lo que permite visualizar la cantidad y diversidad de los clústeres generados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b0442f-148e-49d0-975f-eb41d1079be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agrupación de clusters\n",
    "atm_cluster_map_a = cl.perform_feature_based_clustering(extracted_features_a, n_clusters=8) \n",
    "df_a_clustered = df_a.merge(atm_cluster_map_a, on='atmId', how='left')\n",
    "atm_cluster_map_b = cl.perform_feature_based_clustering(extracted_features_b, n_clusters=12) \n",
    "df_b_clustered = df_b.merge(atm_cluster_map_b, on='atmId', how='left')\n",
    "\n",
    "\n",
    "cluster_labels_a = df_a_clustered['cluster_label'].unique()\n",
    "cluster_labels_b = df_b_clustered['cluster_label'].unique()\n",
    "\n",
    "print(cluster_labels_a)\n",
    "print(cluster_labels_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f97bf6-3b11-4692-881a-62dd2274fd80",
   "metadata": {},
   "source": [
    "## 5) Visualización del Cambio de Balance Agregado por Clúster\n",
    "\n",
    "1. **Propósito de la Función**:\n",
    "   - La función `plot_clustered_time_series` está diseñada para visualizar el comportamiento del cambio de balance agregado por cada clúster de cajeros automáticos (ATMs).\n",
    "   - Los datos se agrupan por hora, y se grafican los cambios de balance promedio para cada clúster.\n",
    "\n",
    "2. **Parámetros**:\n",
    "   - `df`: El DataFrame que contiene los datos de los cajeros automáticos, con las etiquetas de clúster ya asignadas.\n",
    "   - `n_clusters`: El número de clústeres que se desean graficar (por defecto, 5).\n",
    "\n",
    "3. **Proceso**:\n",
    "   - Se determina el tamaño de la cuadrícula de subgráficos (siempre en números pares) según la cantidad de clústeres.\n",
    "   - La función divide los datos en subgráficos, uno por cada clúster, y agrupa los datos por el tiempo de transacción (`transactionTime`) para calcular el cambio de balance promedio (`balance_change`).\n",
    "   - Los datos de cambio de balance se grafican en el formato de línea para cada clúster.\n",
    "\n",
    "4. **Gráficos**:\n",
    "   - Para cada clúster, se crea un gráfico con el cambio de balance agregado a lo largo del tiempo.\n",
    "   - Se ajustan las etiquetas del eje X (fecha) y se rotan para mejorar la legibilidad.\n",
    "   - Se establece el título de cada gráfico con el número del clúster y se agregan etiquetas a los ejes.\n",
    "\n",
    "5. **Visualización Final**:\n",
    "   - Se muestra la visualización ajustada en una cuadrícula de subgráficos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970d39ce-9628-45ca-b6e6-77dc3353dbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clustered_time_series(df, n_clusters=5):\n",
    "    \"\"\"\n",
    "    Plot the aggregated balance change of all ATMs for each cluster, aggregated hourly.\n",
    "    Subplots will be arranged based on the number of clusters.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing ATM data with cluster labels.\n",
    "    - n_clusters: Number of clusters.\n",
    "    \"\"\"\n",
    "    # Determine the grid size (always even)\n",
    "    n_rows = n_clusters // 2\n",
    "    n_cols = 2\n",
    "\n",
    "    # Create subplots with dynamic rows/cols\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, n_rows * 4))\n",
    "\n",
    "    # Flatten the axes for easy iteration in case we have more than 1 row\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for cluster in range(n_clusters):\n",
    "        cluster_data = df[df['cluster_label'] == cluster]\n",
    "        \n",
    "        # Convert transactionTime to datetime and format it\n",
    "        cluster_data['transactionTime'] = pd.to_datetime(cluster_data['transactionTime']).dt.strftime('%Y-%m')\n",
    "\n",
    "        # Group by formatted transactionTime and aggregate balance_change\n",
    "        aggregated_balance_change = cluster_data.groupby('transactionTime')['balance_change'].mean()\n",
    "\n",
    "        # Plot on the appropriate axis\n",
    "        ax = axes[cluster]\n",
    "        ax.plot(aggregated_balance_change.index, aggregated_balance_change.values, label=f'Cluster {cluster+1}')\n",
    "        ax.set_title(f\"Cluster {cluster+1}: Aggregated Balance Change (Hourly)\")\n",
    "        ax.set_xlabel(\"Date\")\n",
    "        ax.set_ylabel(\"Aggregated Balance Change\")\n",
    "        ax.grid(True)\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Adjust the layout to avoid overlap\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bc016c-860f-425c-820d-5897240bd662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comportamiento de los clusters\n",
    "plot_clustered_time_series(df_b_clustered, n_clusters=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacfd378-f238-402c-80b4-57efe0aaaff4",
   "metadata": {},
   "source": [
    "## 6) Procesamiento de Datos por Clúster y Análisis de Clúster 0\n",
    "\n",
    "1. **Propósito del Código**:\n",
    "   - El objetivo es organizar los datos de los cajeros automáticos (ATM) según los clústeres y extraer información detallada del clúster 0.\n",
    "   - Se crea un diccionario para almacenar los DataFrames correspondientes a cada clúster para los conjuntos `df_a_clustered` y `df_b_clustered`.\n",
    "   - Se resetea el índice de cada DataFrame para cada clúster para asegurar que los datos estén bien estructurados.\n",
    "\n",
    "2. **Proceso**:\n",
    "   - Para cada clúster, se filtran los datos y se restablece el índice.\n",
    "   - Los DataFrames resultantes de cada clúster se almacenan en un diccionario (`clustered_dataframes_a` y `clustered_dataframes_b`).\n",
    "   - Además, se extraen datos específicos del clúster 0 de `df_b_clustered` y se imprime la cantidad de registros y cajeros automáticos únicos en ese clúster.\n",
    "   \n",
    "3. **Análisis Específico del Clúster 0**:\n",
    "   - Se extraen los datos del clúster 0 del diccionario `clustered_dataframes_b`.\n",
    "   - Se imprime la cantidad de registros que pertenecen al clúster 0.\n",
    "   - Se calcula y se imprime la cantidad de cajeros automáticos únicos (`atmId`) en el clúster 0.\n",
    "\n",
    "4. **Resultado Final**:\n",
    "   - Los datos se agrupan por clúster, y se puede ver cuántos cajeros automáticos están presentes en cada clúster.\n",
    "   - Se extrae y se muestra información específica para el clúster 0, incluyendo el número de registros y cajeros automáticos únicos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387d47d4-6382-45bf-bce4-a1845213a329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store DataFrames for each cluster\n",
    "clustered_dataframes_a = {}\n",
    "clustered_dataframes_b = {}\n",
    "# Loop over each cluster label and create a DataFrame for each\n",
    "for label in cluster_labels_a:\n",
    "    cluster_df = df_a_clustered[df_a_clustered['cluster_label'] == label]\n",
    "    \n",
    "    # Reset the index for the filtered DataFrame and drop the old index\n",
    "    cluster_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Store the reset DataFrame in the dictionary\n",
    "    clustered_dataframes_a[label] = cluster_df\n",
    "# Optionally, print the number of ATMs in each cluster\n",
    "for label, df_cluster in clustered_dataframes_a.items():\n",
    "    num_atms = df_cluster['atmId'].nunique()\n",
    "    print(f\"Cluster {label}: {num_atms} unique ATMs\")\n",
    "\n",
    "print('DIVISION')\n",
    "for label in cluster_labels_b:\n",
    "    cluster_df = df_b_clustered[df_b_clustered['cluster_label'] == label]\n",
    "    \n",
    "    # Reset the index for the filtered DataFrame and drop the old index\n",
    "    cluster_df.reset_index(drop=True, inplace=True)\n",
    "    # Store the reset DataFrame in the dictionary\n",
    "    clustered_dataframes_b[label] = cluster_df\n",
    "# Optionally, print the number of ATMs in each cluster\n",
    "for label, df_cluster in clustered_dataframes_b.items():\n",
    "    num_atms = df_cluster['atmId'].nunique()\n",
    "    print(f\"Cluster {label}: {num_atms} unique ATMs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d17fb2c-1311-4670-8854-728683d544a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract data for cluster 0\n",
    "cluster_0 = clustered_dataframes_b[0]\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Number of records in cluster 0: {len(cluster_0)}\")\n",
    "print(f\"Number of unique ATMs in cluster 0: {cluster_0['atmId'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2f2f53-4683-4e2a-9b65-e8b68127c3fa",
   "metadata": {},
   "source": [
    "## 7) Procesamiento de Datos por Clúster en `clustered_dataframes_a` y `clustered_dataframes_b`\n",
    "\n",
    "### 1. **Propósito del Código**:\n",
    "   El objetivo principal es procesar los datos de los cajeros automáticos (ATM) agrupados en diferentes clústeres. Este código organiza los datos de cada clúster y genera nuevas características útiles para análisis de series de tiempo, como lags y medias móviles. Se crean dos diccionarios (`processed_data_a` y `processed_data_b`) que contienen los datos procesados para cada clúster.\n",
    "\n",
    "### 2. **Proceso**:\n",
    "   - **Inicialización de diccionarios**: \n",
    "     Se crean dos diccionarios vacíos (`processed_data_a` y `processed_data_b`) para almacenar los resultados procesados para los clústeres de `clustered_dataframes_a` y `clustered_dataframes_b`.\n",
    "   \n",
    "   - **Iteración sobre los Clústeres**:\n",
    "     Se itera sobre cada clúster en `clustered_dataframes_a` y `clustered_dataframes_b`. Para cada clúster:\n",
    "     - Se obtiene la lista de `atmId` únicos presentes en ese clúster.\n",
    "     - Se almacena esta lista de `atmId` en un diccionario asociado a cada clúster.\n",
    "\n",
    "   - **Procesamiento de los Datos por `atmId`**:\n",
    "     - Para cada cajero automático (`atmId`) dentro del clúster:\n",
    "       - Se filtra el DataFrame para obtener solo los registros correspondientes a ese `atmId`.\n",
    "       - Se procesan los datos para crear características de *lag* utilizando la función `gr.process_lag_data`, que identifica los lags más significativos.\n",
    "       - Se calcula una lista de ventanas de tiempo para las medias móviles, basadas en los lags significativos. Si no hay lags significativos, se usan ventanas predeterminadas de 24 horas (diarias) y 168 horas (semanales).\n",
    "       - Se generan nuevas características de medias móviles utilizando la función `gr.create_rolling_mean_features`.\n",
    "\n",
    "   - **Almacenaje de Datos Procesados**:\n",
    "     - Los datos procesados para cada cajero se almacenan dentro del diccionario correspondiente a ese clúster.\n",
    "     - Los diccionarios de datos procesados se almacenan finalmente en `processed_data_a` y `processed_data_b` para todos los clústeres.\n",
    "\n",
    "### 3. **Análisis Específico**:\n",
    "   Aunque el análisis en este código no se centra en un clúster específico, se realiza un procesamiento detallado para cada clúster y cada cajero automático dentro de él. La idea es generar características adicionales (lags y medias móviles) que puedan ser útiles para la predicción de futuros comportamientos de los cajeros automáticos.\n",
    "   El procesamiento de datos incluye el cálculo de lags significativos y la creación de medias móviles que ayudan a modelar el comportamiento temporal de las transacciones en los cajeros automáticos.\n",
    "\n",
    "### 4. **Resultado Final**:\n",
    "   El resultado final es un conjunto de datos enriquecido con características adicionales, como los lags y las medias móviles, que pueden ser utilizados para modelos predictivos.\n",
    "   Los datos de cada cajero se almacenan y organizan por clúster, lo que facilita un análisis más segmentado y específico según las características de cada grupo de cajeros automáticos.\n",
    "   Los datos procesados estarán listos para su posterior análisis o para alimentar modelos de predicción que consideren la temporalidad y los patrones históricos de cada cajero automático.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5d304f-12db-4799-8724-250e813efdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the top-level dictionary to collect processed data for all clusters\n",
    "processed_data_a = {}\n",
    "processed_data_b = {}\n",
    "# Loop over each cluster in clustered_dataframes\n",
    "for cluster_label, cluster_df in clustered_dataframes_a.items():\n",
    "    print(f\"\\nProcessing cluster {cluster_label}\")\n",
    "    \n",
    "    # Initialize the cluster-level dictionary\n",
    "    cluster_dict = {}\n",
    "    \n",
    "    # Get the list of unique atmIds in this cluster\n",
    "    atm_ids = cluster_df['atmId'].unique().tolist()\n",
    "    \n",
    "    # Store the list of ATM IDs in the cluster dictionary\n",
    "    cluster_dict['atm_ids'] = atm_ids\n",
    "    \n",
    "    # Loop over each atmId in the cluster\n",
    "    for atm_id in atm_ids:\n",
    "        print(f\"Processing ATM {atm_id} in cluster {cluster_label}\")\n",
    "        # Get the data for this ATM\n",
    "        atm_data = cluster_df[cluster_df['atmId'] == atm_id].copy()\n",
    "        \n",
    "        # Process ATM data to create lag features\n",
    "        atm_data_processed, significant_lags = gr.process_lag_data(atm_data)\n",
    "        print(f\"Significant lags for ATM {atm_id}: {significant_lags}\")\n",
    "        \n",
    "        # Get significant lags for rolling windows (e.g., lags >= 24)\n",
    "        rolling_windows = [lag for lag in significant_lags if lag >= 24]\n",
    "        \n",
    "        # Ensure some default windows if none are significant\n",
    "        if not rolling_windows:\n",
    "            rolling_windows = [24, 168]  # Daily and weekly\n",
    "        \n",
    "        # Create rolling mean features\n",
    "        atm_data_processed = gr.create_rolling_mean_features(atm_data_processed, rolling_windows)\n",
    "        \n",
    "        # Handle missing values (optional)\n",
    "        #atm_data_processed.dropna(inplace=True)\n",
    "        \n",
    "        # Store the processed DataFrame in the cluster dictionary\n",
    "        cluster_dict[atm_id] = atm_data_processed\n",
    "    \n",
    "    # Add the cluster dictionary to the top-level dictionary\n",
    "    processed_data_a[cluster_label] = cluster_dict\n",
    "for cluster_label, cluster_df in clustered_dataframes_b.items():\n",
    "    print(f\"\\nProcessing cluster {cluster_label}\")\n",
    "    \n",
    "    # Initialize the cluster-level dictionary\n",
    "    cluster_dict = {}\n",
    "    \n",
    "    # Get the list of unique atmIds in this cluster\n",
    "    atm_ids = cluster_df['atmId'].unique().tolist()\n",
    "    \n",
    "    # Store the list of ATM IDs in the cluster dictionary\n",
    "    cluster_dict['atm_ids'] = atm_ids\n",
    "    \n",
    "    # Loop over each atmId in the cluster\n",
    "    for atm_id in atm_ids:\n",
    "        print(f\"Processing ATM {atm_id} in cluster {cluster_label}\")\n",
    "        # Get the data for this ATM\n",
    "        atm_data = cluster_df[cluster_df['atmId'] == atm_id].copy()\n",
    "        \n",
    "        # Process ATM data to create lag features\n",
    "        atm_data_processed, significant_lags = gr.process_lag_data(atm_data)\n",
    "        print(f\"Significant lags for ATM {atm_id}: {significant_lags}\")\n",
    "        \n",
    "        # Get significant lags for rolling windows (e.g., lags >= 24)\n",
    "        rolling_windows = [lag for lag in significant_lags if lag >= 24]\n",
    "        \n",
    "        # Ensure some default windows if none are significant\n",
    "        if not rolling_windows:\n",
    "            rolling_windows = [24, 168]  # Daily and weekly\n",
    "        \n",
    "        # Create rolling mean features\n",
    "        atm_data_processed = gr.create_rolling_mean_features(atm_data_processed, rolling_windows)\n",
    "        \n",
    "        # Handle missing values (optional)\n",
    "        #atm_data_processed.dropna(inplace=True)\n",
    "        \n",
    "        # Store the processed DataFrame in the cluster dictionary\n",
    "        cluster_dict[atm_id] = atm_data_processed\n",
    "    \n",
    "    # Add the cluster dictionary to the top-level dictionary\n",
    "    processed_data_b[cluster_label] = cluster_dict\n",
    "print(\"\\nProcessing complete.\")\n",
    "processed_data_a\n",
    "processed_data_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306f6a42-c8ad-4b9f-b9f5-b9be537081cd",
   "metadata": {},
   "source": [
    "##  8) Procesamiento de Datos por Clúster en `processed_data_agg_a` y `processed_data_agg_b`\n",
    "\n",
    "### 1. **Propósito del Código**:\n",
    "   El objetivo principal de este código es procesar los datos de los cajeros automáticos (ATM) por clústeres. Se agregan los cambios de balance por hora, se crean características adicionales relacionadas con los lags significativos y se generan medias móviles. Los resultados procesados se almacenan en dos diccionarios, `processed_data_agg_a` y `processed_data_agg_b`, para cada uno de los clústeres de los DataFrames `clustered_dataframes_a` y `clustered_dataframes_b`.\n",
    "\n",
    "### 2. **Proceso**:\n",
    "   - **Inicialización de diccionarios**:\n",
    "     Se crean dos diccionarios vacíos: `processed_data_agg_a` y `processed_data_agg_b` para almacenar los resultados procesados de cada clúster.\n",
    "\n",
    "   - **Iteración sobre los clústeres**:\n",
    "     El proceso se realiza de manera similar para ambos conjuntos de clústeres (`clustered_dataframes_a` y `clustered_dataframes_b`):\n",
    "     - **Extracción de `atmId`**:\n",
    "       Se obtiene una lista de los `atmId` únicos presentes en cada clúster, lo que permite identificar los cajeros automáticos en cada grupo.\n",
    "     \n",
    "     - **Reagregación de datos**:\n",
    "       Se agrupan los datos de las transacciones por la columna `transactionTime`, sumando los valores de `balance_change` para cada intervalo de tiempo (hora).\n",
    "       Esto genera una serie temporal del cambio de balance por hora, agregando los datos de todos los cajeros automáticos del clúster.\n",
    "\n",
    "     - **Procesamiento de datos de lags**:\n",
    "       Se utiliza la función `gr.process_lag_data` para procesar los datos y obtener los lags significativos para la serie temporal de `balance_change`.\n",
    "\n",
    "     - **Selección de ventanas para medias móviles**:\n",
    "       Se filtran los lags significativos que sean mayores o iguales a 24, lo que permite identificar las ventanas de tiempo relevantes para las medias móviles. Si no hay lags significativos, se utilizan ventanas predeterminadas de 24 horas (diarias) y 168 horas (semanales).\n",
    "\n",
    "     - **Generación de medias móviles**:\n",
    "       Se genera una serie de características de medias móviles utilizando la función `gr.create_rolling_mean_features`, basándose en las ventanas seleccionadas.\n",
    "\n",
    "     - **Almacenaje de los datos procesados**:\n",
    "       Los datos procesados se almacenan en los diccionarios correspondientes, con información del clúster (`cluster_number`), los `atm_ids` del clúster y los datos procesados.\n",
    "\n",
    "### 3. **Análisis Específico**:\n",
    "   Este código no realiza un análisis específico de cada clúster, pero sí prepara los datos para un análisis posterior. Se procesan características temporales y de cambio de balance, lo que permite comprender el comportamiento de los cajeros automáticos a lo largo del tiempo. Además, se incorporan lags y medias móviles, que pueden ser útiles para modelos predictivos.\n",
    "\n",
    "### 4. **Resultado Final**:\n",
    "   El resultado final es un conjunto de datos procesados que incluye las características agregadas por hora, lags significativos y medias móviles. Estos datos pueden ser utilizados en análisis posteriores o como entrada para modelos predictivos.\n",
    "   \n",
    "   Los datos de cada clúster se organizan y se almacenan en los diccionarios `processed_data_agg_a` y `processed_data_agg_b`, donde están listos para su análisis y modelado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6ed81d-f830-409d-b2de-9e3249ba542a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the top-level dictionary to collect processed data for all clusters\n",
    "processed_data_agg_a = {}\n",
    "processed_data_agg_b = {}\n",
    "for cluster_label, cluster_df in clustered_dataframes_a.items():\n",
    "    print(f\"\\nProcessing cluster {cluster_label}\")\n",
    "    \n",
    "    # Get the list of unique atmIds in this cluster\n",
    "    atm_ids = cluster_df['atmId'].unique().tolist()\n",
    "    \n",
    "    print(cluster_df.head())\n",
    "    # Resample the data to hourly intervals, summing 'balance_change'\n",
    "    # This gives us a time series of balance_change per hour, aggregated over all ATMs in the cluster\n",
    "    cluster_aggregated = cluster_df.groupby(['transactionTime'])['balance_change'].sum()\n",
    "    print(type(cluster_aggregated))\n",
    "    print(cluster_aggregated)\n",
    "    # Reset index to have 'transactionTime' as a column\n",
    "    cluster_aggregated_df = cluster_aggregated.reset_index()\n",
    "    print(cluster_aggregated_df.head())\n",
    "    cluster_data_processed, significant_lags = gr.process_lag_data(cluster_aggregated_df)\n",
    "    print(significant_lags)\n",
    "    # Get significant lags for rolling windows (e.g., lags >= 24)\n",
    "    rolling_windows = [lag for lag in significant_lags if lag >= 24]\n",
    "    \n",
    "    # Ensure some default windows if none are significant\n",
    "    if not rolling_windows:\n",
    "        rolling_windows = [24, 168]  # Daily and weekly\n",
    "    \n",
    "    # Create rolling mean features\n",
    "    cluster_data_processed = gr.create_rolling_mean_features(cluster_data_processed, rolling_windows)\n",
    "    \n",
    "    # Handle missing values\n",
    "    #cluster_data_processed.dropna(inplace=True)\n",
    "    \n",
    "    # Store the data in the processed_data dictionary\n",
    "    processed_data_agg_a[cluster_label] = {\n",
    "        'cluster_number': cluster_label,\n",
    "        'atm_ids': atm_ids,\n",
    "        'data': cluster_data_processed\n",
    "    }\n",
    "\n",
    "print(\"\\nProcessing complete.\")\n",
    "\n",
    "for cluster_label, cluster_df in clustered_dataframes_b.items():\n",
    "    print(f\"\\nProcessing cluster {cluster_label}\")\n",
    "    \n",
    "    # Get the list of unique atmIds in this cluster\n",
    "    atm_ids = cluster_df['atmId'].unique().tolist()\n",
    "    \n",
    "    print(cluster_df.head())\n",
    "    # Resample the data to hourly intervals, summing 'balance_change'\n",
    "    # This gives us a time series of balance_change per hour, aggregated over all ATMs in the cluster\n",
    "    cluster_aggregated = cluster_df.groupby(['transactionTime'])['balance_change'].sum()\n",
    "    print(type(cluster_aggregated))\n",
    "    print(cluster_aggregated)\n",
    "    # Reset index to have 'transactionTime' as a column\n",
    "    cluster_aggregated_df = cluster_aggregated.reset_index()\n",
    "    print(cluster_aggregated_df.head())\n",
    "    cluster_data_processed, significant_lags = gr.process_lag_data(cluster_aggregated_df)\n",
    "    print(significant_lags)\n",
    "    # Get significant lags for rolling windows (e.g., lags >= 24)\n",
    "    rolling_windows = [lag for lag in significant_lags if lag >= 24]\n",
    "    \n",
    "    # Ensure some default windows if none are significant\n",
    "    if not rolling_windows:\n",
    "        rolling_windows = [24, 168]  # Daily and weekly\n",
    "    \n",
    "    # Create rolling mean features\n",
    "    cluster_data_processed = gr.create_rolling_mean_features(cluster_data_processed, rolling_windows)\n",
    "    \n",
    "    # Handle missing values\n",
    "    #cluster_data_processed.dropna(inplace=True)\n",
    "    \n",
    "    # Store the data in the processed_data dictionary\n",
    "    processed_data_agg_b[cluster_label] = {\n",
    "        'cluster_number': cluster_label,\n",
    "        'atm_ids': atm_ids,\n",
    "        'data': cluster_data_processed\n",
    "    }\n",
    "\n",
    "print(\"\\nProcessing complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf92dae7-16c0-47a1-9f9f-e654599a2eaf",
   "metadata": {},
   "source": [
    "## 9) Creación de Series de Tiempo Agregadas para 'a', 'b' y 'a+b'\n",
    "\n",
    "### 1. **Propósito del Código**:\n",
    "   El objetivo de este código es agregar los datos de los clústeres para los grupos 'a', 'b' y 'a+b'. Para cada uno de estos grupos:\n",
    "   - Se agregan los cambios de balance por hora.\n",
    "   - Se procesan los datos generando características de lags y medias móviles.\n",
    "   - Se almacenan los resultados procesados en tres diccionarios distintos.\n",
    "\n",
    "### 2. **Proceso**:\n",
    "\n",
    "   - **Inicialización de Diccionarios**:\n",
    "     Se crean tres diccionarios vacíos: `processed_data_total_a`, `processed_data_total_b`, y `processed_data_total_ab`, para almacenar los datos agregados de cada grupo.\n",
    "\n",
    "   - **Agregación de los Clústeres en 'a'**:\n",
    "     Para cada clúster en el grupo 'a':\n",
    "     - Se obtiene y concatena los datos de los clústeres.\n",
    "     - Se realiza una agregación por la columna `transactionTime`, sumando la columna `balance_change`.\n",
    "     - Se procesan los lags significativos y se generan características de medias móviles.\n",
    "     - Los datos procesados se almacenan en el diccionario `processed_data_total_a`.\n",
    "\n",
    "   - **Agregación de los Clústeres en 'b'**:\n",
    "Se repite el mismo proceso que para 'a', pero para el grupo 'b', almacenando los resultados en `processed_data_total_b`.\n",
    "\n",
    "   - **Agregación de los Grupos 'a' y 'b' para Obtener 'a+b'**:\n",
    "     Se fusionan los datos agregados de los grupos 'a' y 'b' en una sola tabla por la columna `transactionTime`.\n",
    "     - Se suman las columnas `balance_change_a` y `balance_change_b` para obtener la columna combinada `balance_change`.\n",
    "     - Se procesan los lags y se crean características de medias móviles.\n",
    "     - Los datos procesados se almacenan en el diccionario `processed_data_total_ab`.\n",
    "\n",
    "   - **Diccionario Final con los Tres Grupos**:\n",
    "     Finalmente, se almacenan los tres grupos ('a', 'b', 'a+b') en un solo diccionario llamado `processed_data_total`.\n",
    "\n",
    "### 3. **Resultado Final**:\n",
    "   El código devuelve un diccionario final, `processed_data_total`, que contiene los datos procesados de cada grupo, listos para análisis o modelado posterior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e08c0d4-e02c-454a-8ee4-3a85eb030426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear las series de tiempo agregadas para 'a', 'b' y 'a+b'\n",
    "\n",
    "# Importar librerías necesarias\n",
    "import pandas as pd\n",
    "\n",
    "# Diccionarios para almacenar los datos agregados\n",
    "processed_data_total_a = {}\n",
    "processed_data_total_b = {}\n",
    "processed_data_total_ab = {}\n",
    "\n",
    "# Agregar todos los clústeres en 'a'\n",
    "print(\"\\nAgregando todos los clústeres en el grupo 'a'\")\n",
    "\n",
    "# Lista para almacenar los dataframes de cada clúster en 'a'\n",
    "cluster_dataframes_a = []\n",
    "\n",
    "for cluster_info in processed_data_agg_a.values():\n",
    "    cluster_data_processed = cluster_info['data']\n",
    "    cluster_dataframes_a.append(cluster_data_processed[['transactionTime', 'balance_change']])\n",
    "\n",
    "# Concatenar todos los dataframes de los clústeres en 'a'\n",
    "all_data_a = pd.concat(cluster_dataframes_a)\n",
    "\n",
    "# Sumar 'balance_change' por 'transactionTime'\n",
    "total_aggregated_a = all_data_a.groupby('transactionTime')['balance_change'].sum().reset_index()\n",
    "\n",
    "# Procesar los datos agregados como antes\n",
    "cluster_data_processed_a, significant_lags_a = gr.process_lag_data(total_aggregated_a)\n",
    "\n",
    "# Obtener lags significativos para las ventanas móviles (por ejemplo, lags >= 24)\n",
    "rolling_windows_a = [lag for lag in significant_lags_a if lag >= 24]\n",
    "\n",
    "# Asegurar ventanas predeterminadas si no hay lags significativos\n",
    "if not rolling_windows_a:\n",
    "    rolling_windows_a = [24, 168]  # Diaria y semanal\n",
    "\n",
    "# Crear características de media móvil\n",
    "cluster_data_processed_a = gr.create_rolling_mean_features(cluster_data_processed_a, rolling_windows_a)\n",
    "\n",
    "# Almacenar los datos en el diccionario\n",
    "processed_data_total_a = {\n",
    "    'group': 'a',\n",
    "    'data': cluster_data_processed_a\n",
    "}\n",
    "\n",
    "# Agregar todos los clústeres en 'b'\n",
    "print(\"\\nAgregando todos los clústeres en el grupo 'b'\")\n",
    "\n",
    "# Lista para almacenar los dataframes de cada clúster en 'b'\n",
    "cluster_dataframes_b = []\n",
    "\n",
    "for cluster_info in processed_data_agg_b.values():\n",
    "    cluster_data_processed = cluster_info['data']\n",
    "    cluster_dataframes_b.append(cluster_data_processed[['transactionTime', 'balance_change']])\n",
    "\n",
    "# Concatenar todos los dataframes de los clústeres en 'b'\n",
    "all_data_b = pd.concat(cluster_dataframes_b)\n",
    "\n",
    "# Sumar 'balance_change' por 'transactionTime'\n",
    "total_aggregated_b = all_data_b.groupby('transactionTime')['balance_change'].sum().reset_index()\n",
    "\n",
    "# Procesar los datos agregados como antes\n",
    "cluster_data_processed_b, significant_lags_b = gr.process_lag_data(total_aggregated_b)\n",
    "\n",
    "# Obtener lags significativos para las ventanas móviles\n",
    "rolling_windows_b = [lag for lag in significant_lags_b if lag >= 24]\n",
    "\n",
    "# Asegurar ventanas predeterminadas si no hay lags significativos\n",
    "if not rolling_windows_b:\n",
    "    rolling_windows_b = [24, 168]  # Diaria y semanal\n",
    "\n",
    "# Crear características de media móvil\n",
    "cluster_data_processed_b = gr.create_rolling_mean_features(cluster_data_processed_b, rolling_windows_b)\n",
    "\n",
    "# Almacenar los datos en el diccionario\n",
    "processed_data_total_b = {\n",
    "    'group': 'b',\n",
    "    'data': cluster_data_processed_b\n",
    "}\n",
    "\n",
    "# Agregar los grupos 'a' y 'b' para obtener 'a+b'\n",
    "print(\"\\nAgregando los grupos 'a' y 'b' para obtener 'a+b'\")\n",
    "\n",
    "# Fusionar los datos agregados de 'a' y 'b' en 'transactionTime'\n",
    "total_aggregated_ab = pd.merge(\n",
    "    total_aggregated_a,\n",
    "    total_aggregated_b,\n",
    "    on='transactionTime',\n",
    "    how='outer',\n",
    "    suffixes=('_a', '_b')\n",
    ")\n",
    "\n",
    "# Reemplazar NaN con 0\n",
    "total_aggregated_ab.fillna(0, inplace=True)\n",
    "\n",
    "# Sumar las columnas 'balance_change' de 'a' y 'b'\n",
    "total_aggregated_ab['balance_change'] = total_aggregated_ab['balance_change_a'] + total_aggregated_ab['balance_change_b']\n",
    "\n",
    "# Mantener solo 'transactionTime' y 'balance_change'\n",
    "total_aggregated_ab = total_aggregated_ab[['transactionTime', 'balance_change']]\n",
    "\n",
    "# Procesar los datos agregados como antes\n",
    "cluster_data_processed_ab, significant_lags_ab = gr.process_lag_data(total_aggregated_ab)\n",
    "\n",
    "# Obtener lags significativos para las ventanas móviles\n",
    "rolling_windows_ab = [lag for lag in significant_lags_ab if lag >= 24]\n",
    "\n",
    "# Asegurar ventanas predeterminadas si no hay lags significativos\n",
    "if not rolling_windows_ab:\n",
    "    rolling_windows_ab = [24, 168]  # Diaria y semanal\n",
    "\n",
    "# Crear características de media móvil\n",
    "cluster_data_processed_ab = gr.create_rolling_mean_features(cluster_data_processed_ab, rolling_windows_ab)\n",
    "\n",
    "# Almacenar los datos en el diccionario\n",
    "processed_data_total_ab = {\n",
    "    'group': 'a+b',\n",
    "    'data': cluster_data_processed_ab\n",
    "}\n",
    "\n",
    "# Almacenar todos los datos agregados en un diccionario final\n",
    "processed_data_total = {\n",
    "    'a': processed_data_total_a,\n",
    "    'b': processed_data_total_b,\n",
    "    'a+b': processed_data_total_ab\n",
    "}\n",
    "\n",
    "print(\"\\nAgregación completa.\")\n",
    "\n",
    "processed_data_total['a+b']\n",
    "\n",
    "\n",
    "# Optionally, print the number of ATMs in each cluster\n",
    "for label, df_cluster in clustered_dataframes_a.items():\n",
    "    num_atms = df_cluster['atmId'].nunique()\n",
    "    print(f\"Cluster {label}: {num_atms} unique ATMs\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3dd13d-c890-40e3-b214-7ec9f6ae610a",
   "metadata": {},
   "source": [
    "#  10) Predicción de datos de cajeros automáticos (ATMs)\n",
    "\n",
    "Este script se encarga de realizar un análisis y predicción de datos de cajeros automáticos (ATMs) mediante la aplicación de varios modelos de Machine Learning, incluyendo LightGBM y AdaBoost. A continuación se describen los pasos principales:\n",
    "\n",
    "### 1. **Definición de los Clústeres**\n",
    "   Se define un conjunto de clústeres `group_clusters` donde cada clúster tiene:\n",
    "   - Un conjunto de índices (`indices`) que corresponde a diferentes grupos de datos.\n",
    "   - Un conjunto de datos procesados asociados a cada grupo (por ejemplo, `processed_data_a` y `processed_data_b`).\n",
    "\n",
    "### 2. **Bucle de Procesamiento por Clúster**\n",
    "   Se itera sobre cada clúster de `group_clusters`, accediendo a las configuraciones y datos de cada grupo:\n",
    "   - **Acceso a los Datos**: Para cada clúster, se accede a los datos correspondientes y se maneja la imprecisión de los índices para evitar errores si algún índice está fuera de los límites.\n",
    "   - **Imputación de Valores Faltantes**: Para cada ATM dentro del clúster, se realiza la imputación de valores faltantes usando el método KNN con una cantidad de vecinos definida.\n",
    "\n",
    "### 3. **Entrenamiento de Modelos**\n",
    "   Para cada ATM dentro de un clúster, se entrenan los siguientes modelos de Machine Learning:\n",
    "   - **LightGBM**: Se entrena un modelo LightGBM usando las características seleccionadas del dataset. Además, se realiza una predicción en el futuro (con los siguientes `n_hours`).\n",
    "   - **AdaBoost**: De manera similar, se entrena un modelo AdaBoost con las mismas características y se realiza una predicción futura.\n",
    "\n",
    "### 4. **Ensamble de Modelos**\n",
    "   Para mejorar la precisión de las predicciones, se realiza un ensamble de los modelos LightGBM y AdaBoost:\n",
    "   - Se calculan las predicciones de ambos modelos y se obtiene un valor promedio de las mismas.\n",
    "\n",
    "### 5. **Evaluación de Modelos**\n",
    "   Los resultados de los modelos son evaluados utilizando dos métricas de desempeño:\n",
    "   - **RMSE (Root Mean Squared Error)**: Mide la diferencia cuadrada promedio entre las predicciones y los valores reales.\n",
    "   - **MAPE (Mean Absolute Percentage Error)**: Mide el error relativo entre las predicciones y los valores reales.\n",
    "\n",
    "### 6. **Almacenamiento de Resultados**\n",
    "   Los resultados de cada ATM y cada modelo se almacenan en un diccionario `atm_results_group`, que se guarda en un archivo `.pkl` usando `joblib` al finalizar el procesamiento de todos los clústeres.\n",
    "\n",
    "### 7. **Consideraciones**\n",
    "   - **Manejo de Errores**: En cada paso de la iteración, se manejan posibles errores que podrían surgir al acceder a los datos o al entrenar los modelos.\n",
    "   - **Predicciones Futuras**: Se realizan predicciones para las siguientes `n_hours` y se almacenan las predicciones del futuro.\n",
    "   - **Optimización de Parámetros**: El código también incluye la optimización de parámetros en el entrenamiento de los modelos, como los hiperparámetros del LightGBM.\n",
    "\n",
    "### Resultados:\n",
    "Los resultados incluyen:\n",
    "- **RMSE** y **MAPE** para cada ATM y cada modelo (LightGBM, AdaBoost y Ensamble).\n",
    "- Predicciones futuras para cada ATM.\n",
    "- Los resultados se guardan en un archivo `atm_results_cluster_{group_label}_batch.pkl` para su posterior análisis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559f30ee-368d-4c60-98c4-00dc935918e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='lightgbm')\n",
    "import joblib\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "# Definir tus grupos de clústeres y los datos procesados asociados\n",
    "group_clusters = {\n",
    "    'a1': {\n",
    "        'indices': [3, 0, 2, 5],  # aux_a_1\n",
    "        'data': processed_data_a,\n",
    "    },\n",
    "    'a2': {\n",
    "        'indices': [1, 7, 4, 6],  # aux_a_2\n",
    "        'data': processed_data_a,\n",
    "    },\n",
    "    'b1': {\n",
    "        'indices': [8, 2, 11, 6, 4, 5],  # aux_b_1\n",
    "        'data': processed_data_b,\n",
    "    },\n",
    "    'b2': {\n",
    "        'indices': [0, 1, 3, 7, 9, 10],  # aux_b_2\n",
    "        'data': processed_data_b,\n",
    "    },\n",
    "    'b2_2': {\n",
    "        'indices': [7, 9, 10],  # aux_b_2_2\n",
    "        'data': processed_data_b,\n",
    "    },\n",
    "}\n",
    "\n",
    "n_hours = 28\n",
    "\n",
    "# Bucle sobre cada grupo\n",
    "for group_label, group_info in group_clusters.items():\n",
    "    cluster_indices = group_info['indices']\n",
    "    processed_data = group_info['data']\n",
    "    atm_results_group = defaultdict(dict)\n",
    "    \n",
    "    for i in cluster_indices:\n",
    "        # Acceder a los datos del clúster de manera segura\n",
    "        try:\n",
    "            single_cluster_label, single_cluster_data = list(processed_data.items())[i]\n",
    "        except IndexError:\n",
    "            print(f\"El índice {i} está fuera de los límites para los datos procesados en el grupo {group_label}.\")\n",
    "            continue\n",
    "        \n",
    "        atm_ids = single_cluster_data['atm_ids']\n",
    "        for atm_id in atm_ids:\n",
    "            print(f\"\\nProcesando ATM: {atm_id} en el clúster {single_cluster_label}\")\n",
    "            atm_df = single_cluster_data[atm_id].copy()\n",
    "            aux_1 = p.columns_with_na(atm_df)\n",
    "            atm_df = p.impute_missing_values_knn(atm_df, aux_1, 'atmId', n_neighbors=3)\n",
    "            \n",
    "            # Asegurarse de que 'transactionTime' es datetime\n",
    "            atm_df['transactionTime'] = pd.to_datetime(atm_df['transactionTime'])\n",
    "            atm_df, lam, shift = aplicar_boxcox(atm_df, atm_id)\n",
    "            # Ordenar por 'transactionTime'\n",
    "            atm_df = atm_df.sort_values('transactionTime').reset_index(drop=True)\n",
    "    \n",
    "            # Definir características (excluir columnas específicas)\n",
    "            features = [col for col in atm_df.columns if col not in ['balance_change', 'balance_change_boxcox', 'transactionTime', 'atmId', 'tipo_cajero']]\n",
    "\n",
    "            # Incluir 'balance_change_boxcox' como variable objetivo\n",
    "            target = 'balance_change_boxcox'\n",
    "            # Preparar datos de entrenamiento y prueba\n",
    "            last_day = '2024-05-14'\n",
    "            test_start_time = pd.to_datetime(str(last_day))\n",
    "            train_df = atm_df[atm_df['transactionTime'] < test_start_time]\n",
    "            test_df = atm_df[atm_df['transactionTime'] >= test_start_time]\n",
    "    \n",
    "            # Entrenar LightGBM\n",
    "            try:\n",
    "                rmse_lgbm, mape_lgbm, y_pred_lgbm_boxcox, best_params_lgbm, model_lgbm = md.train_lightgbm(train_df, test_df, features, target)\n",
    "                future_pred_lgbm_boxcox, future_df_lgbm = md.rolling_predictions_with_lag_update(\n",
    "                    model_lgbm, 'lgbm', atm_df, features, n_hours=n_hours, target=target\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"LightGBM falló para el ATM {atm_id}: {e}\")\n",
    "                rmse_lgbm, mape_lgbm, y_pred_lgbm_boxcox, best_params_lgbm, future_pred_lgbm_boxcox = None, None, [], {}, []\n",
    "            \n",
    "            if y_pred_lgbm_boxcox:\n",
    "                y_pred_lgbm = destransformar_boxcox(np.array(y_pred_lgbm_boxcox), lam, shift)\n",
    "            else:\n",
    "                y_pred_lgbm = []\n",
    "\n",
    "            if future_pred_lgbm_boxcox:\n",
    "                future_pred_lgbm = destransformar_boxcox(np.array(future_pred_lgbm_boxcox), lam, shift)\n",
    "            else:\n",
    "                future_pred_lgbm = []\n",
    "            # Entrenar AdaBoost\n",
    "            try:\n",
    "                rmse_ada, mape_ada, y_pred_ada_boxcox, best_params_ada, model_ada = md.train_adaboost(train_df, test_df, features, target)\n",
    "                future_pred_ada_boxcox, future_df_ada = md.rolling_predictions_with_lag_update(\n",
    "                    model_ada, 'adaboost', atm_df, features, n_hours=n_hours, target=target\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"AdaBoost falló para el ATM {atm_id}: {e}\")\n",
    "                rmse_ada, mape_ada, y_pred_ada_boxcox, best_params_ada, future_pred_ada_boxcox = None, None, [], {}, []\n",
    "\n",
    "            # Destransformar las predicciones\n",
    "            if y_pred_ada_boxcox:\n",
    "                y_pred_ada = destransformar_boxcox(np.array(y_pred_ada_boxcox), lam, shift)\n",
    "            else:\n",
    "                y_pred_ada = []\n",
    "\n",
    "            if future_pred_ada_boxcox:\n",
    "                future_pred_ada = destransformar_boxcox(np.array(future_pred_ada_boxcox), lam, shift)\n",
    "            else:\n",
    "                future_pred_ada = []\n",
    "            # Ensamble - Promedio\n",
    "            try:\n",
    "                predictions_list = [np.array(preds) for preds in [y_pred_lgbm, y_pred_ada] if len(preds) > 0]\n",
    "                if predictions_list:\n",
    "                    y_pred_ensemble = md.averaging_ensemble_predictions(predictions_list)\n",
    "                    rmse_ensemble = mean_squared_error(test_df['balance_change'], y_pred_ensemble, squared=False)\n",
    "                    mape_ensemble = mean_absolute_percentage_error(test_df['balance_change'], y_pred_ensemble)\n",
    "                else:\n",
    "                    raise ValueError(\"No hay predicciones válidas para el ensamble.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Ensamble falló para el ATM {atm_id}: {e}\")\n",
    "                rmse_ensemble, mape_ensemble, y_pred_ensemble = None, None, []\n",
    "    \n",
    "            # Guardar resultados\n",
    "            atm_results_group[atm_id] = {\n",
    "                'LightGBM': {\n",
    "                    'RMSE': rmse_lgbm,\n",
    "                    'MAPE': mape_lgbm,\n",
    "                    'predictions': y_pred_lgbm if y_pred_lgbm else [],\n",
    "                    'best_params': best_params_lgbm,\n",
    "                    'future_predictions': future_pred_lgbm,\n",
    "                },\n",
    "                'AdaBoost': {\n",
    "                    'RMSE': rmse_ada,\n",
    "                    'MAPE': mape_ada,\n",
    "                    'predictions': y_pred_ada if y_pred_ada else [],\n",
    "                    'best_params': best_params_ada,\n",
    "                    'future_predictions': future_pred_ada,\n",
    "                },\n",
    "                'Ensemble_Averaging': {\n",
    "                    'RMSE': rmse_ensemble,\n",
    "                    'MAPE': mape_ensemble,\n",
    "                    'predictions': y_pred_ensemble.tolist() if isinstance(y_pred_ensemble, np.ndarray) else []\n",
    "                },\n",
    "            }\n",
    "                \n",
    "    # Mostrar los resultados\n",
    "    for atm_id, model_results in atm_results_group.items():\n",
    "        print(f\"\\nATM ID: {atm_id}\")\n",
    "        for model_name, metrics in model_results.items():\n",
    "            print(f\"Modelo: {model_name}, RMSE: {metrics['RMSE']}, MAPE: {metrics['MAPE']}\")\n",
    "    \n",
    "    # Guardar los resultados\n",
    "    joblib.dump(atm_results_group, f'atm_results_cluster_{group_label}_batch.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b71ebc4-8a50-4767-bb34-8543a4543415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='lightgbm')\n",
    "import joblib\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "# Combinar los datos agregados de ambos grupos\n",
    "processed_data_agg = {\n",
    "    'a': processed_data_agg_a,  # 8 clústeres\n",
    "    'b': processed_data_agg_b,  # 12 clústeres\n",
    "}\n",
    "\n",
    "n_hours = 28  # Número de horas para predicciones futuras\n",
    "\n",
    "# Diccionario para almacenar los resultados de todos los clústeres\n",
    "cluster_results = {}\n",
    "\n",
    "# Bucle sobre cada grupo ('a' y 'b')\n",
    "for group_label, group_data in processed_data_agg.items():\n",
    "    # Bucle sobre cada clúster en el grupo\n",
    "    for cluster_number, cluster_data in group_data.items():\n",
    "        print(f\"\\nProcesando clúster {group_label}{cluster_number}\")\n",
    "\n",
    "        # Extraer el dataframe agregado del clúster\n",
    "        cluster_df = cluster_data['data'].copy()\n",
    "        \n",
    "        # Asegurarse de que 'transactionTime' es datetime\n",
    "        cluster_df['transactionTime'] = pd.to_datetime(cluster_df['transactionTime'])\n",
    "        \n",
    "        # Imputación de valores faltantes si es necesario\n",
    "        aux_1 = p.columns_with_na(cluster_df)\n",
    "        if aux_1:\n",
    "            cluster_df = p.impute_missing_values_knn(cluster_df, aux_1, 'transactionTime', n_neighbors=3)\n",
    "        \n",
    "        # Ordenar por 'transactionTime'\n",
    "        cluster_df = cluster_df.sort_values('transactionTime').reset_index(drop=True)\n",
    "        \n",
    "        # Definir características (excluir columnas específicas)\n",
    "        features = [col for col in cluster_df.columns if col not in ['balance_change', 'transactionTime']]\n",
    "        \n",
    "        # Preparar datos de entrenamiento y prueba\n",
    "        last_day = '2024-05-14'\n",
    "        test_start_time = pd.to_datetime(str(last_day))\n",
    "        train_df = cluster_df[cluster_df['transactionTime'] < test_start_time]\n",
    "        test_df = cluster_df[cluster_df['transactionTime'] >= test_start_time]\n",
    "        \n",
    "        # Entrenar LightGBM\n",
    "        try:\n",
    "            rmse_lgbm, mape_lgbm, y_pred_lgbm, best_params_lgbm, model_lgbm = md.train_lightgbm(train_df, test_df, features)\n",
    "            future_pred_lgbm, future_df_lgbm = md.rolling_predictions_with_lag_update(\n",
    "                model_lgbm, 'lgbm', cluster_df, features, n_hours=n_hours\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"LightGBM falló para el clúster {group_label}{cluster_number}: {e}\")\n",
    "            rmse_lgbm, mape_lgbm, y_pred_lgbm, best_params_lgbm, future_pred_lgbm = None, None, [], {}, []\n",
    "        \n",
    "        # Entrenar AdaBoost\n",
    "        try:\n",
    "            rmse_ada, mape_ada, y_pred_ada, best_params_ada, model_ada = md.train_adaboost(train_df, test_df, features)\n",
    "            future_pred_ada, future_df_ada = md.rolling_predictions_with_lag_update(\n",
    "                model_ada, 'adaboost', cluster_df, features, n_hours=n_hours\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"AdaBoost falló para el clúster {group_label}{cluster_number}: {e}\")\n",
    "            rmse_ada, mape_ada, y_pred_ada, best_params_ada, future_pred_ada = None, None, [], {}, []\n",
    "        \n",
    "        # Ensamble - Promedio\n",
    "        try:\n",
    "            predictions_list = [np.array(preds) for preds in [y_pred_lgbm, y_pred_ada] if len(preds) > 0]\n",
    "            if predictions_list:\n",
    "                y_pred_ensemble = md.averaging_ensemble_predictions(predictions_list)\n",
    "                rmse_ensemble = mean_squared_error(test_df['balance_change'], y_pred_ensemble, squared=False)\n",
    "                mape_ensemble = mean_absolute_percentage_error(test_df['balance_change'], y_pred_ensemble)\n",
    "            else:\n",
    "                raise ValueError(\"No hay predicciones válidas para el ensamble.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Ensamble falló para el clúster {group_label}{cluster_number}: {e}\")\n",
    "            rmse_ensemble, mape_ensemble, y_pred_ensemble = None, None, []\n",
    "        \n",
    "        # Guardar resultados en el diccionario\n",
    "        cluster_key = f\"{group_label}{cluster_number}\"\n",
    "        cluster_results[cluster_key] = {\n",
    "            'LightGBM': {\n",
    "                'RMSE': rmse_lgbm,\n",
    "                'MAPE': mape_lgbm,\n",
    "                'predictions': y_pred_lgbm if y_pred_lgbm else [],\n",
    "                'best_params': best_params_lgbm,\n",
    "                'future_predictions': future_pred_lgbm,\n",
    "            },\n",
    "            'AdaBoost': {\n",
    "                'RMSE': rmse_ada,\n",
    "                'MAPE': mape_ada,\n",
    "                'predictions': y_pred_ada if y_pred_ada else [],\n",
    "                'best_params': best_params_ada,\n",
    "                'future_predictions': future_pred_ada,\n",
    "            },\n",
    "            'Ensemble_Averaging': {\n",
    "                'RMSE': rmse_ensemble,\n",
    "                'MAPE': mape_ensemble,\n",
    "                'predictions': y_pred_ensemble.tolist() if isinstance(y_pred_ensemble, np.ndarray) else []\n",
    "            },\n",
    "        }\n",
    "        \n",
    "        # Mostrar los resultados\n",
    "        print(f\"\\nResultados para el clúster {group_label}{cluster_number}:\")\n",
    "        for model_name, metrics in cluster_results[cluster_key].items():\n",
    "            print(f\"Modelo: {model_name}, RMSE: {metrics['RMSE']}, MAPE: {metrics['MAPE']}\")\n",
    "        \n",
    "# Guardar los resultados en un archivo\n",
    "joblib.dump(cluster_results, 'cluster_results_predictions.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76afa347-ea6e-477a-8015-928c0c601f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='lightgbm')\n",
    "import joblib\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "# Utilizar los datos agregados de las series 'a', 'b' y 'a+b'\n",
    "processed_data_total = {\n",
    "    'a': processed_data_total_a,     # Serie agregada 'a'\n",
    "    'b': processed_data_total_b,     # Serie agregada 'b'\n",
    "    'a+b': processed_data_total_ab   # Serie agregada 'a+b'\n",
    "}\n",
    "\n",
    "n_hours = 28  # Número de horas para predicciones futuras\n",
    "\n",
    "# Diccionario para almacenar los resultados de las series agregadas\n",
    "total_series_results = {}\n",
    "\n",
    "# Bucle sobre cada serie agregada ('a', 'b', 'a+b')\n",
    "for series_label, series_data in processed_data_total.items():\n",
    "    print(f\"\\nProcesando serie agregada '{series_label}'\")\n",
    "\n",
    "    # Extraer el dataframe de la serie agregada\n",
    "    series_df = series_data['data'].copy()\n",
    "    \n",
    "    # Asegurarse de que 'transactionTime' es datetime\n",
    "    series_df['transactionTime'] = pd.to_datetime(series_df['transactionTime'])\n",
    "    \n",
    "    # Imputación de valores faltantes si es necesario\n",
    "    aux_1 = p.columns_with_na(series_df)\n",
    "    if aux_1:\n",
    "        series_df = p.impute_missing_values_knn(series_df, aux_1, 'transactionTime', n_neighbors=3)\n",
    "    \n",
    "    # Ordenar por 'transactionTime'\n",
    "    series_df = series_df.sort_values('transactionTime').reset_index(drop=True)\n",
    "    \n",
    "    # Definir características (excluir columnas específicas)\n",
    "    features = [col for col in series_df.columns if col not in ['balance_change', 'transactionTime']]\n",
    "    \n",
    "    # Preparar datos de entrenamiento y prueba\n",
    "    last_day = '2024-05-14'\n",
    "    test_start_time = pd.to_datetime(str(last_day))\n",
    "    train_df = series_df[series_df['transactionTime'] < test_start_time]\n",
    "    test_df = series_df[series_df['transactionTime'] >= test_start_time]\n",
    "    \n",
    "    # Entrenar LightGBM\n",
    "    try:\n",
    "        rmse_lgbm, mape_lgbm, y_pred_lgbm, best_params_lgbm, model_lgbm = md.train_lightgbm(train_df, test_df, features)\n",
    "        future_pred_lgbm, future_df_lgbm = md.rolling_predictions_with_lag_update(\n",
    "            model_lgbm, 'lgbm', series_df, features, n_hours=n_hours\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"LightGBM falló para la serie '{series_label}': {e}\")\n",
    "        rmse_lgbm, mape_lgbm, y_pred_lgbm, best_params_lgbm, future_pred_lgbm = None, None, [], {}, []\n",
    "    \n",
    "    # Entrenar AdaBoost\n",
    "    try:\n",
    "        rmse_ada, mape_ada, y_pred_ada, best_params_ada, model_ada = md.train_adaboost(train_df, test_df, features)\n",
    "        future_pred_ada, future_df_ada = md.rolling_predictions_with_lag_update(\n",
    "            model_ada, 'adaboost', series_df, features, n_hours=n_hours\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"AdaBoost falló para la serie '{series_label}': {e}\")\n",
    "        rmse_ada, mape_ada, y_pred_ada, best_params_ada, future_pred_ada = None, None, [], {}, []\n",
    "    \n",
    "    # Ensamble - Promedio\n",
    "    try:\n",
    "        predictions_list = [np.array(preds) for preds in [y_pred_lgbm, y_pred_ada] if len(preds) > 0]\n",
    "        if predictions_list:\n",
    "            y_pred_ensemble = md.averaging_ensemble_predictions(predictions_list)\n",
    "            rmse_ensemble = mean_squared_error(test_df['balance_change'], y_pred_ensemble, squared=False)\n",
    "            mape_ensemble = mean_absolute_percentage_error(test_df['balance_change'], y_pred_ensemble)\n",
    "        else:\n",
    "            raise ValueError(\"No hay predicciones válidas para el ensamble.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ensamble falló para la serie '{series_label}': {e}\")\n",
    "        rmse_ensemble, mape_ensemble, y_pred_ensemble = None, None, []\n",
    "    \n",
    "    # Guardar resultados en el diccionario\n",
    "    total_series_results[series_label] = {\n",
    "        'LightGBM': {\n",
    "            'RMSE': rmse_lgbm,\n",
    "            'MAPE': mape_lgbm,\n",
    "            'predictions': y_pred_lgbm if y_pred_lgbm else [],\n",
    "            'best_params': best_params_lgbm,\n",
    "            'future_predictions': future_pred_lgbm,\n",
    "        },\n",
    "        'AdaBoost': {\n",
    "            'RMSE': rmse_ada,\n",
    "            'MAPE': mape_ada,\n",
    "            'predictions': y_pred_ada if y_pred_ada else [],\n",
    "            'best_params': best_params_ada,\n",
    "            'future_predictions': future_pred_ada,\n",
    "        },\n",
    "        'Ensemble_Averaging': {\n",
    "            'RMSE': rmse_ensemble,\n",
    "            'MAPE': mape_ensemble,\n",
    "            'predictions': y_pred_ensemble.tolist() if isinstance(y_pred_ensemble, np.ndarray) else []\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    # Mostrar los resultados\n",
    "    print(f\"\\nResultados para la serie agregada '{series_label}':\")\n",
    "    for model_name, metrics in total_series_results[series_label].items():\n",
    "        print(f\"Modelo: {model_name}, RMSE: {metrics['RMSE']}, MAPE: {metrics['MAPE']}\")\n",
    "        \n",
    "# Guardar los resultados en un archivo entendible\n",
    "joblib.dump(total_series_results, 'total_series_results_predictions.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69af9fd-cf7f-47b5-9048-0549fe962ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608eadc6-a935-4406-8344-e497566364c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "atm_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94c0c9e-50f7-49eb-be4e-bc427aadc7f8",
   "metadata": {},
   "source": [
    "## 11) Cálculo de la Matriz de Covarianza de las Predicciones\n",
    "\n",
    "Este bloque de código tiene como objetivo extraer las predicciones generadas por el modelo de ensamblaje promedio (Ensemble Averaging) para diferentes niveles de agregación: total, grupos, clústeres y cajeros automáticos (ATMs), y almacenarlas en diccionarios separados.\n",
    "\n",
    "1. **Extraer las predicciones totales (para la serie 'a+b'):**\n",
    "   - Se extraen las predicciones del modelo de Ensemble_Averaging para la serie agregada 'a+b' desde el diccionario `total_series_results`. Luego, estas predicciones se convierten a un arreglo de Numpy para facilitar su posterior análisis.\n",
    "\n",
    "2. **Extraer las predicciones por grupos (para las series 'a' y 'b'):**\n",
    "   - Se extraen las predicciones del modelo de Ensemble_Averaging para las series individuales 'a' y 'b', y se almacenan en un diccionario llamado `group_forecasts`. Cada clave en este diccionario corresponde a una serie ('a' o 'b').\n",
    "\n",
    "3. **Extraer las predicciones por clústeres:**\n",
    "   - Se recorre cada clúster dentro de `cluster_results` y se extraen las predicciones correspondientes. Estas predicciones se almacenan en el diccionario `cluster_forecasts`, donde cada clave es un identificador de clúster.\n",
    "\n",
    "4. **Extraer las predicciones por cajero automático (ATM):**\n",
    "   - Similar al bloque anterior, pero en este caso se recorren los cajeros automáticos en `atm_results` y se extraen las predicciones correspondientes. Estas predicciones se almacenan en el diccionario `atm_forecasts`, donde las claves son los identificadores de los cajeros automáticos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a466ab-e04f-4842-901e-de7ae3aabe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Extract total forecasts\n",
    "total_forecast = np.array(total_series_results['a+b']['Ensemble_Averaging']['predictions'])\n",
    "\n",
    "# Extract group forecasts\n",
    "group_forecasts = {\n",
    "    'a': np.array(total_series_results['a']['Ensemble_Averaging']['predictions']),\n",
    "    'b': np.array(total_series_results['b']['Ensemble_Averaging']['predictions'])\n",
    "}\n",
    "\n",
    "# Extract cluster forecasts\n",
    "cluster_forecasts = {}\n",
    "for cluster_key, data in cluster_results.items():\n",
    "    cluster_forecasts[cluster_key] = np.array(data['Ensemble_Averaging']['predictions'])\n",
    "\n",
    "# Extract ATM forecasts\n",
    "atm_forecasts = {}\n",
    "for atm_id, data in atm_results.items():\n",
    "    atm_forecasts[atm_id] = np.array(data['Ensemble_Averaging']['predictions'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95997bc4-9423-4100-8ec2-83419d49431e",
   "metadata": {},
   "source": [
    "## 12)  Matriz para la reconciliación \n",
    "\n",
    "El siguiente código tiene como objetivo construir una matriz de diseño utilizada para la reconciliación de las predicciones a diferentes niveles de agregación (total, grupos, clústeres y cajeros automáticos). Aquí se describen los pasos clave.\n",
    "\n",
    "1. **Definición de las Variables de Entrada**\n",
    "   - **atm_ids**: Lista que contiene todos los identificadores de los cajeros automáticos (ATMs).\n",
    "   - **cluster_keys**: Lista con los identificadores de los clústeres.\n",
    "   - **group_keys**: Lista con los identificadores de los grupos ('a' y 'b').\n",
    "   - **time_steps**: Número de pasos de tiempo, determinado por la longitud de las predicciones totales.\n",
    "\n",
    "2. **Creación de los Mapeos**\n",
    "   - **atm_to_cluster**: Diccionario que mapea cada cajero automático (ATM) a su clúster correspondiente. Para construir este mapeo, se recorren los datos agregados de los grupos 'a' y 'b', extrayendo los cajeros automáticos asociados con cada clúster.\n",
    "   - **cluster_to_group**: Diccionario que mapea cada clúster a su grupo correspondiente (ya sea 'a' o 'b'). Esto se realiza también recorriendo los datos agregados de los grupos y clústeres.\n",
    "\n",
    "3. **Verificación de la Asignación de los Cajeros Automáticos a los Clústeres**\n",
    "   - El código verifica si todos los cajeros automáticos tienen asignado un clúster. Si algún cajero automático no está asignado, se imprime una advertencia con una lista de los cajeros automáticos no mapeados.\n",
    "\n",
    "4. **Determinación del Número Total de Series**\n",
    "   - Se calcula el número total de series que serán incluidas en la matriz de diseño. Este número es la suma de las series a nivel total (1), los grupos, los clústeres y los cajeros automáticos.\n",
    "\n",
    "5. **Construcción de la Matriz de Diseño \\( S \\)**\n",
    "   La matriz de diseño \\( S \\) se utiliza para reflejar las relaciones jerárquicas entre los diferentes niveles de agregación y reconciliar las predicciones. Tiene una fila para cada nivel de agregación y columnas para cada grupo, clúster y cajero automático.\n",
    "\n",
    "   - **Total**: La primera fila representa el nivel total y tiene un valor de 1 en la columna \"Total\".\n",
    "   - **Grupos**: Para cada grupo ('a' y 'b'), se agrega una fila donde:\n",
    "     - Se asigna un valor de 1 en la columna \"Total\" (porque el grupo contribuye al total).\n",
    "     - Se asigna un valor de -1 en la columna correspondiente al grupo (porque la predicción del grupo se debe restar para reconciliar).\n",
    "   - **Clústeres**: Para cada clúster, se agrega una fila donde:\n",
    "     - Se asigna un valor de 1 en el grupo correspondiente (porque el clúster contribuye a su grupo).\n",
    "     - Se asigna un valor de -1 en la columna del clúster (porque la predicción del clúster se debe restar para reconciliar).\n",
    "   - **Cajeros Automáticos (ATMs)**: Para cada cajero automático, se agrega una fila donde:\n",
    "     - Se asigna un valor de 1 en el clúster correspondiente (porque el cajero automático contribuye a su clúster).\n",
    "     - Se asigna un valor de -1 en la columna del cajero automático (porque la predicción del cajero automático se debe restar para reconciliar).\n",
    "\n",
    "6. **Advertencia para Cajeros Automáticos No Asignados**\n",
    "   - Si algún cajero automático no tiene asignado un clúster, el código imprime una advertencia y omite ese cajero automático en la construcción de la matriz de diseño.\n",
    "\n",
    "### Resumen Final:\n",
    "Este código organiza las predicciones de los cajeros automáticos (ATMs) en una estructura jerárquica, lo que permite reconciliar las predicciones a diferentes niveles de agregación mediante una matriz de diseño. Esta matriz refleja cómo cada nivel (total, grupo, clúster, ATM) contribuye o resta para calcular las predicciones finales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ff1af7-c81f-4d66-8a79-686b33f54a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of all ATM IDs\n",
    "atm_ids = list(atm_forecasts.keys())\n",
    "\n",
    "# List of clusters\n",
    "cluster_keys = list(cluster_forecasts.keys())\n",
    "\n",
    "# List of groups\n",
    "group_keys = list(group_forecasts.keys())\n",
    "\n",
    "# Number of time steps\n",
    "time_steps = len(total_forecast)\n",
    "\n",
    "# Build the mapping from ATMs to clusters\n",
    "atm_to_cluster = {}\n",
    "\n",
    "# Combine processed_data_agg_a and processed_data_agg_b\n",
    "processed_data_agg = {'a': processed_data_agg_a, 'b': processed_data_agg_b}\n",
    "\n",
    "for group_label, group_data in processed_data_agg.items():\n",
    "    for cluster_label, cluster_info in group_data.items():\n",
    "        cluster_key = f\"{group_label}{cluster_label}\"\n",
    "        atm_ids_in_cluster = cluster_info['atm_ids']\n",
    "        for atm_id in atm_ids_in_cluster:\n",
    "            atm_to_cluster[atm_id] = cluster_key\n",
    "\n",
    "# Build the mapping from clusters to groups\n",
    "cluster_to_group = {}\n",
    "for group_label, group_data in processed_data_agg.items():\n",
    "    for cluster_label in group_data.keys():\n",
    "        cluster_key = f\"{group_label}{cluster_label}\"\n",
    "        cluster_to_group[cluster_key] = group_label\n",
    "\n",
    "# Verify that all ATMs have been mapped\n",
    "unmapped_atms = set(atm_ids) - set(atm_to_cluster.keys())\n",
    "if unmapped_atms:\n",
    "    print(f\"Warning: The following ATMs are not mapped to any cluster: {unmapped_atms}\")\n",
    "\n",
    "# Total number of series\n",
    "n_total_series = 1 + len(group_keys) + len(cluster_keys) + len(atm_ids)\n",
    "\n",
    "# Build the design matrix S (aggregation matrix)\n",
    "S = pd.DataFrame(0, index=range(n_total_series), columns=['Total'] + group_keys + cluster_keys + atm_ids)\n",
    "\n",
    "# Row indices\n",
    "row_idx = 0\n",
    "\n",
    "# Total level\n",
    "S.loc[row_idx, 'Total'] = 1  # Total is itself\n",
    "row_idx += 1\n",
    "\n",
    "# Group level\n",
    "for group in group_keys:\n",
    "    S.loc[row_idx, 'Total'] = 1    # Each group contributes to the total\n",
    "    S.loc[row_idx, group] = -1     # Subtract group forecast to reconcile\n",
    "    row_idx += 1\n",
    "\n",
    "# Cluster level\n",
    "for cluster in cluster_keys:\n",
    "    group = cluster_to_group[cluster]\n",
    "    S.loc[row_idx, group] = 1        # Each cluster contributes to its group\n",
    "    S.loc[row_idx, cluster] = -1     # Subtract cluster forecast to reconcile\n",
    "    row_idx += 1\n",
    "\n",
    "# ATM level\n",
    "for atm_id in atm_ids:\n",
    "    cluster = atm_to_cluster.get(atm_id)\n",
    "    if cluster:\n",
    "        S.loc[row_idx, cluster] = 1    # Each ATM contributes to its cluster\n",
    "        S.loc[row_idx, atm_id] = -1    # Subtract ATM forecast to reconcile\n",
    "        row_idx += 1\n",
    "    else:\n",
    "        print(f\"ATM {atm_id} is not assigned to any cluster. Skipping in design matrix.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7814fbf-5bf1-43ba-9680-b0c0b7d13345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert S to a NumPy array\n",
    "S_matrix = S.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80427209-8689-42ab-b26b-a222a3e29e0d",
   "metadata": {},
   "source": [
    "## 13)  Matriz de covarianza basada en los errores\n",
    "\n",
    "El objetivo de este bloque de código es apilar las predicciones de diferentes niveles (total, grupo, clúster, cajero automático) en un solo vector para cada paso de tiempo y calcular la matriz de covarianza basada en los errores de predicción de los cajeros automáticos (ATMs).\n",
    "\n",
    "1. **Apilamiento de las Predicciones en un Solo Vector**\n",
    "   - **base_forecasts**: Se inicializa como una lista vacía para almacenar las predicciones apiladas a lo largo de los pasos de tiempo.\n",
    "   - Para cada paso de tiempo (definido por `time_steps`), se crea una lista `forecasts_t` que almacena las predicciones de los distintos niveles:\n",
    "     - **Total forecast**: Se agrega la predicción total correspondiente al paso de tiempo actual.\n",
    "     - **Group forecasts**: Se agregan las predicciones para cada grupo (`group_keys`).\n",
    "     - **Cluster forecasts**: Se agregan las predicciones para cada clúster (`cluster_keys`).\n",
    "     - **ATM forecasts**: Se agregan las predicciones para cada cajero automático (`atm_ids`) solo si tiene errores calculados.\n",
    "   - Después de agregar todas las predicciones a `forecasts_t`, esta lista se agrega a `base_forecasts`, que se convierte en un arreglo de `NumPy` con forma `(time_steps, n_total_series)`.\n",
    "\n",
    "2. **Cálculo de la Matriz de Covarianza**\n",
    "   - **Función `calculate_cov_matrix`**: Esta función calcula la matriz de covarianza basada en los errores de las predicciones de los cajeros automáticos.\n",
    "     - **Entradas**:\n",
    "       - `atm_forecasts`: Un diccionario que contiene las predicciones de los cajeros automáticos.\n",
    "       - `historical_actuals`: Un diccionario que contiene los valores reales históricos de los cajeros automáticos.\n",
    "     - **Proceso**:\n",
    "       - Para cada cajero automático (`atm_id`), se verifica si tiene datos reales en `historical_actuals`. Si no se encuentra, se imprime un mensaje de advertencia y se omite ese cajero.\n",
    "       - Se alinean las predicciones y los datos reales para asegurar que tengan la misma longitud.\n",
    "       - Se calculan los errores de predicción como la diferencia entre los valores reales y las predicciones.\n",
    "       - Los errores se almacenan en `forecast_errors` y los cajeros automáticos con errores se almacenan en `atm_ids_with_errors`.\n",
    "     - **Cálculo de la matriz de covarianza**: Después de apilar los errores, se calcula la matriz de covarianza utilizando `np.cov`. Si no hay suficientes datos (menos de 2 cajeros automáticos con errores calculados), se lanza una excepción.\n",
    "   - **Salida**:\n",
    "     - La matriz de covarianza `cov_matrix`, que describe la relación entre los errores de predicción de los cajeros automáticos.\n",
    "     - `atm_ids_with_errors`, que es una lista de los cajeros automáticos que tienen errores calculados.\n",
    "\n",
    "3. **Actualización de los Cajeros Automáticos y sus Predicciones**\n",
    "   - Después de calcular la matriz de covarianza, se actualizan las listas `atm_ids` y `atm_forecasts` para incluir solo aquellos cajeros automáticos que tienen errores calculados. Esto garantiza que solo se utilicen los cajeros automáticos con datos relevantes en análisis posteriores.\n",
    "\n",
    "### Resumen Final:\n",
    "Este bloque de código organiza y apila las predicciones de los distintos niveles jerárquicos (total, grupo, clúster, cajero automático) para cada paso de tiempo, y luego calcula la matriz de covarianza basada en los errores de predicción de los cajeros automáticos, proporcionando una forma de evaluar la relación entre los errores en las predicciones y ajustarlas en consecuencia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a43054-79b7-4bcb-afd7-f48ae4e505fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack forecasts into a single vector for each time step\n",
    "base_forecasts = []\n",
    "for t in range(time_steps):\n",
    "    forecasts_t = []\n",
    "\n",
    "    # Total forecast\n",
    "    forecasts_t.append(total_forecast[t])\n",
    "\n",
    "    # Group forecasts\n",
    "    for group in group_keys:\n",
    "        forecasts_t.append(group_forecasts[group][t])\n",
    "\n",
    "    # Cluster forecasts\n",
    "    for cluster in cluster_keys:\n",
    "        forecasts_t.append(cluster_forecasts[cluster][t])\n",
    "\n",
    "    # ATM forecasts (only those with errors calculated)\n",
    "    for atm_id in atm_ids:\n",
    "        forecasts_t.append(atm_forecasts[atm_id][t])\n",
    "\n",
    "    base_forecasts.append(forecasts_t)\n",
    "\n",
    "base_forecasts = np.array(base_forecasts)  # Shape: (time_steps, n_total_series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ca73bf-486d-412e-81fe-0012d58310f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7b00c8-63fa-4cfa-aa9c-008da75feeab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b42a60-3984-4650-97a4-d1cb19d855c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
